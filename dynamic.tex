Full formal proof of correctness is, at present, impractical for most realistic systems, and especially the kind of modest-budget, time constrainted flight projects that make use of FPrime.  The actual work of fault detection and validation of software still relies, fundamentally, on effective testing.  Modeling and static approaches to correctness often must rest on a basis of numerous un-examined assumptions about the behavior of hardware systems and low-level system behavior.  Only actual concrete inputs can be executed on real hardware, and satisfy regulatory requirements on code coverage such as those imposed on civilian avionics by DO-178B, etc.~\cite{MCDC}.  Furthermore, only testing can prove faults are not spurious, the result of imprecise abstraction or weak assumptions.

%DeepState~\cite{DeepState} is a dynamic analysis tool that aims to provide a single, usable, flexible front-end to a wide variety of back-end systems for test generation.
Most developers do not know how to use symbolic execution tools; developers seldom even know how to use less challenging tools such as gray-box fuzzers, even relatively push-button ones such as AFL~\cite{aflfuzz}.  Even those developers whose primary focus is critical security infrastructure such as OpenSSL are often not users, much less expert users, of such tools, and embedded systems developers are almost never adept users of modern fuzzing systems.  Furthermore, different tools find different faults, have different scalability limitations, and even have different show-stopping bugs that prevent them from being applied to specific testing problems.  DeepState~\cite{DeepState,DeepStateTutorial} addresses these problems.  First, developers \emph{do}, usually, know how to use unit testing frameworks, such as JUnit~\cite{JUnit} or Google Test~\cite{GoogleTest}. DeepState makes it possible to write parameterized unit tests~\cite{ParamUnit} in a GoogleTest-like framework, and automatically produce tests using symbolic execution tools~\cite{angr1,angr2,angr3,Manticore}, or fuzzers like AFL~\cite{aflfuzz} or libFuzzer~\cite{libFuzzer} (as well as Eclipser~\cite{eclipser}, Angora~\cite{angora}, and Honggfuzz~\cite{Honggfuzz}).  DeepState targets the same space as property-based testing tools such as QuickCheck~\cite{ClaessenH00}, ScalaCheck~\cite{ScalaCheckDoc}, Hypothesis~\cite{Hypothesis}, and TSTL~\cite{NFM15,tstlsttt}, but for C/C++ unit tests. DeepState is, most importantly, the first tool to provide a front-end that can make use of a growing variety of back-ends for test generation.  Developers who write tests using DeepState can expect that DeepState will let them, without rewriting their tests, make use of new symbolic execution or fuzzing advances.  The harness/test definition remains the same, but the method(s) used to generate tests may change over time.  Most property-based tools only provide random testing, and symbolic execution tools such as Pex~\cite{Pex,UnitMeister} or KLEE~\cite{KLEE} offer only a single back-end.
DeepState has already been used to test (and find bugs in) an ext3-like file system~\cite{testfs,testfsrepo} and a widely used compression library, and a summer intern (from Northern Arizona University) at JPL explored using DeepState to test FPrime-based software in the summer of 2023.  Although only released in early 2018, DeepState is already one of the most popular property-based testing and fuzzing projects on GitHub, and has been used internally by both startups and well-established companies, and in security audits by Trail of Bits.  There have even been informal discussions of integrating DeepState, once matured, into a future release of the GoogleTest~\cite{GoogleTest} platform.  PI Groce is at present the lead developer for DeepState.

Alastair Reid and others from Google Research~\cite{meeting} have argued that the essential task for those researching formal methods is ``meeting developers where they are,'' that is, making it integerating formal methods with existing practices and workflows.  This applies not only to formal methods, but to automated test generation and other software correctness technologies that tend to require additional specification effort or tool expertise.  In fact, given the ongoing scalability limits of full verification, especially for budget and time constrained efforts such as many NASA missions or typical industrial embedded systems projects, the idea is even more applicable to methods with higher immediate payoff and lower need for specialized expertise, such as automated test generation.  The core insight behind this project is as follows:

\begin{itemize}
\item Embedded systems frameworks such as FPrime are \emph{already in use} by developers, because they offer such direct benefits as reduced development time and cost and increased reusability of code across projects.
\item Such frameworks \emph{also} provide (and \emph{force}) additional specification and semantic effort, either by requiring developers to make choices indicating additional behavior constraints during development explicitly, or by offering implementations with known and exploitable semantics and structure.  This means they are able to guide advanced software correctness technologies, both in terms of constraining the meaningful input space to be explored and in terms of offering additional criteria to detect flaws in the code (additional, low-cost to acquire, information on behavior constraints).
\item Furthermore, use of frameworks often involves additional tooling and process knowledge; if software correctness technologies can be smoothly integrated into these already-known aspects of development using a framework, they can be perceived as extremely low-cost.
\end{itemize}

In short, ``piggybacking'' on a framework makes software correctness technologies much lower in perceived-cost to developers; they have already determined to use a framework for other reasons, and, in the embedded systems world, often operate with a desire for correctness and reliability.  Making advanced correctness methods (e.g., fuzzer-based automated test generation, and sophisticated mutation analysis) available at minimal extra cost in terms of developer effort, but with additional extra benefit (in terms of effectiveness of the technologies) offers the potential to dramatically increase the usage of such technologies.  Moreover, the additional usage is not a dubious proposition in terms of economic/mission success cost-benefit ratios:  the proposal is to let developers adopt software correctness technologies when they are least costly and likely to be most beneficial.  We further speculate that more developers adopting software correctness technologies will make the actual core limitations (and thus potential research directions) of an essentially unsolvable problem, that of software correctness, more visible.

This proposal therefore aims to advance four thrusts, all targeting integration of advanced software correctness technologies with embedded systems frameworks:

\begin{enumerate}
\item First, we aim to make it possible for developers to use \emph{automatically generated} DeepState harnesses to fuzz (and symbolically explore, when feasible) components of embedded systems.  Using the information on, e.g., FPrime ports, and other constraints known from the framework semantics will make it possible to improve the effectiveness of test generation in terms of efficient bug-finding and lower the number of false positives due to inputs that do not represent real-world software behaviors.
\item Second, we aim to enhance both automated and manual testing by exploiting semantics from frameworks to \emph{automatically generate additional runtime-checkable specification information}.  As a primary practical example that has long been of interest, but has a surprisingly poor history of practical implementation, we offer up \emph{modifies clauses} that can check for many memory corruptions, perhaps the most important class of bugs in C and C++ software systems.
\item Third, we propose to explore framework-specific mutation generation, making it easier for developers to \emph{know if their current correctness checks are sufficiently strong or if they may miss important bugs.}
\item Finally, we propose to explore not just test generation but full model checking for some components, making use of the clean structure of framework-based code to identify individual software components suitable for the more thorough, but more limited and less scalable, correctness check offered by full verification.
\end{enumerate}

The intellectual value of these contributions is that all demonstrate the value of moving from ``generic'' tooling for software correctness to more powerful, lower-cost methods that are integrated with suitable software frameworks.  We believe this work can serve to inspire other efforts, in other domains where frameworks and standard software architectures can make it both easier and more effective to apply advanced correctness technologies.